{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import db_builder.db_handler as dbh\n",
    "import numpy as np\n",
    "from torchvision import transforms, datasets\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "from getpass import getpass\n",
    "from PIL import Image\n",
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import os\n",
    "import cv2"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "52a012dbd410515c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logging.getLogger('sqlalchemy').setLevel(logging.WARNING)\n",
    "db_params = {\n",
    "    'user': 'postgres',\n",
    "    'password': getpass('Please enter DB pw'),  # enter your DB password\n",
    "    'host': 'localhost',  # 'localhost' or IP address\n",
    "    'port': '5432',  # '5432'\n",
    "    'database': 'ttdatabase',  #tensionTerminator\n",
    "}\n",
    "toolcheck = dbh.DB_Conn(db_params)\n",
    "toolcheck.connect()\n",
    "engine = toolcheck.get_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def trigger_crop(image):\n",
    "    crop_box = (400, 450, 550, 550)\n",
    "    cropped_image = transforms.functional.crop(image, *crop_box)\n",
    "    return cropped_image\n",
    "\n",
    "\n",
    "def video_to_image_converter(source_path: str, output_path: str, crop=False):\n",
    "    vidcap = cv2.VideoCapture(source_path)\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "    count = 0\n",
    "\n",
    "    total_frames = int(vidcap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    #fps = int(vidcap.get(cv2.CAP_PROP_FPS))\n",
    "    fps = 30\n",
    "\n",
    "    while count < total_frames:\n",
    "        success, image = vidcap.read()\n",
    "\n",
    "        if count % int(fps) == 0:\n",
    "            if not image is None and not image.size == 0:\n",
    "                transform_test = transforms.ToPILImage()\n",
    "                image = transform_test(image)\n",
    "                if crop:\n",
    "                    image = trigger_crop(image)\n",
    "                image = np.asarray(image)\n",
    "                cv2.imwrite(f\"{output_path}/{count}.png\", image)  # save frame as PNG file\n",
    "\n",
    "        count += 1\n",
    "\n",
    "    vidcap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    \n",
    "def checkImage(path: str, transfer_model, orig_set, transforms_wt):\n",
    "\n",
    "    img = Image.open(path)\n",
    "    img_tensor = transforms_wt(img).unsqueeze(0)\n",
    "    prediction = transfer_model(img_tensor.to(\"cuda\"))\n",
    "    predicted_probabilities = torch.softmax(prediction, dim=1)\n",
    "    predicted_class_idx = torch.argmax(prediction).item()\n",
    "    predicted_class = orig_set.classes[predicted_class_idx]\n",
    "    \n",
    "\n",
    "    # Get the confidence score for the predicted class\n",
    "    confidence = predicted_probabilities[0, predicted_class_idx].item() * 100  # Convert to percentage\n",
    "\n",
    "    return predicted_class, confidence\n",
    "\n",
    "\n",
    "transforms_wt = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        [0.485, 0.456, 0.406],\n",
    "        [0.229, 0.224, 0.225]\n",
    "    )\n",
    "])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8ac040173d7f8ffc"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "directory = \"tmp\"\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "    \n",
    "video_source = toolcheck.get_filepath_by_loop_id(146)\n",
    "video_to_image_converter(video_source, directory, True)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "37a9db5fb33c9dac"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "orig_set = datasets.ImageFolder(\n",
    "    root='C:\\\\Users\\\\Pirmin.000\\\\PycharmProjects\\\\IGP\\\\data\\\\tool_finder\\\\10_11_2023_21_05_33',\n",
    "    transform=transforms_wt\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f8ab0260bd8599fc"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "orig_set.classes"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "54598793073f1600"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "transfer_model = models.resnet152()\n",
    "transfer_model.fc = nn.Sequential(\n",
    "    nn.Linear(transfer_model.fc.in_features, 2048),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(),\n",
    "    nn.Linear(2048,1024),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(),\n",
    "    nn.Linear(1024,500),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(),\n",
    "    nn.Linear(500,2)\n",
    ")\n",
    "transfer_model_state_dict = torch.load(\"C:\\\\Users\\\\Pirmin.000\\\\PycharmProjects\\\\IGP\\\\models\\\\tool_finder\\\\10_11_2023_21_05_33\\\\model.pt\")\n",
    "transfer_model.load_state_dict(transfer_model_state_dict)\n",
    "transfer_model.to(\"cuda\")\n",
    "transfer_model.eval()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4924529851cf275a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dataset = []\n",
    "conf = []\n",
    "directory = \"tmp\"\n",
    "\n",
    "for filename in tqdm(os.listdir(directory)):\n",
    "    f = os.path.join(directory, filename)\n",
    "    #print(f)\n",
    "    predicted_class, confidence = checkImage(f, transfer_model, orig_set, transforms_wt)\n",
    "    conf.append(confidence)\n",
    "    dataset.append(predicted_class)\n",
    "\n",
    "    os.remove(f)\n",
    "os.rmdir(directory)\n",
    "dataset_confidence = sum(conf)/len(conf)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "82c878d0973219d4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dataset_confidence"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b4f5621e2d8f0b8d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from collections import Counter"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2b17c4e382447d02"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test = Counter(dataset)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "266d64e487cb43fa"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "97fabe7efb79b164"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test[orig_set.classes[0]]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "67309ee887a8352d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test[orig_set.classes[1]]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "51a99ab11b93cd59"
  },
  {
   "cell_type": "markdown",
   "source": [
    "<video controls src=video_source />"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ad92f4a05917542d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "video_source"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "22673e737b78c689"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
